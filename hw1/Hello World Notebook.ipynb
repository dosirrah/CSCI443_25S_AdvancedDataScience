{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "108310e9-711c-4782-8e94-5a0c29db98ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Hello World\n",
    "\n",
    "This is an example intended just to get you using the tools that will be used in this class.\n",
    "\n",
    "This is standard Markdown used by many tools including but not limited to Google colab, jupyter notebooks, github, and bitbucket.\n",
    "\n",
    "```\n",
    "# this is a code block\n",
    "x = 10\n",
    "print(x)\n",
    "```\n",
    "\n",
    "Example table using markdown\n",
    "\n",
    "|x  | y   |\n",
    "|---|-----|\n",
    "| 1 | 12  |\n",
    "| 2 | 15  |\n",
    "| 3 | 7   |\n",
    "\n",
    "Example equation using LaTeX\n",
    "\n",
    "$$\\int_{x=0}^\\infty x^2 dx$$\n",
    "\n",
    "Another Example $$x^3$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3c9fa8e-25f8-4d5f-84a6-1f0ea3944699",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Upload train.csv\n",
    "\n",
    "To do this select \"File -> Upload data to dbfs\" and then click on \"Drop files to upload or click to browse\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1c67cb5-1b9f-444e-abc0-45160abc5401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "display(dbutils.fs.ls(\"/FileStore/shared_uploads/harrison@cs.olemiss.edu/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0e8d00c-c7cf-490f-a7e6-46d0acbc3b98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"dbfs:/FileStore/shared_uploads/harrison@cs.olemiss.edu/train.csv\"\n",
    "file_content = dbutils.fs.head(file_path, 1000) # Adjust the second parameter as needed based on the file size\n",
    "print(file_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f38baa86-547a-4990-8e1a-c066f8c6a521",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 4: Use a DataFrame\n",
    "\n",
    "Extend the \"Hello World\" notebook from wihtin Databricks to load `train.csv` into a DataFrame.  Outputthe first 10\n",
    "\n",
    "From the Titanic dataset `train.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9503f6d1-b9d5-4184-be21-b12c80ffd4f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# df = pd.read_csv(file_path)\n",
    "file_path = \"/dbfs/FileStore/shared_uploads/harrison@cs.olemiss.edu/train.csv\"\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "683f8e79-2ece-4054-8471-e08c02aa2e98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Original DBFS file path\n",
    "dbfs_file_path = \"dbfs:/FileStore/shared_uploads/harrison@cs.olemiss.edu/train.csv\"\n",
    "\n",
    "# Create a temporary file\n",
    "with tempfile.NamedTemporaryFile() as temp_file:\n",
    "    # Copy the file from DBFS to the local file system using dbutils\n",
    "    dbutils.fs.cp(dbfs_file_path, \"file:\" + temp_file.name)\n",
    "\n",
    "    # Read the CSV file into a Pandas DataFrame\n",
    "    df = pd.read_csv(temp_file.name)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26bb60fd-63c7-4c41-a65e-6cfd3862640c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Instead of copying the file from dbfs to use read_csv on a local file, we can \n",
    "use a Spark DataFrame.  Spark will read from dbfs directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "284604bb-58a8-47de-b476-e07a72673a46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbfs_file_path = \"dbfs:/FileStore/shared_uploads/harrison@cs.olemiss.edu/train.csv\"\n",
    "\n",
    "# Read CSV file into a Spark DataFrame\n",
    "df = spark.read.csv(dbfs_file_path, header=True, inferSchema=True)\n",
    "df.show(n=10)  # df.show() is analogous to df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74c51290-54cf-4599-a8fb-a40d0e569cb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "As an alternative to using a Spark DataFrame, we can use Spark to create a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1790ffe-0108-451a-ad21-0abcde5a6cfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "dbfs_file_path = \"dbfs:/FileStore/shared_uploads/harrison@cs.olemiss.edu/train.csv\"\n",
    "\n",
    "# Initialize SparkSession (In Databricks, SparkSession is already initialized for you as 'spark')\n",
    "spark = SparkSession.builder.appName(\"myApp\").getOrCreate()\n",
    "\n",
    "# Read the file from DBFS into a Spark DataFrame\n",
    "# Replace 'path/to/your/file.csv' with the actual path to your file in DBFS\n",
    "spark_df = spark.read.csv(dbfs_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Convert the Spark DataFrame to a Pandas DataFrame\n",
    "pandas_df = spark_df.toPandas()\n",
    "\n",
    "# Now you can work with the Pandas DataFrame as usual\n",
    "pandas_df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d61f3c03-e48b-4511-b004-043bbf6c0753",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Unfortunately downloading to the local filesystem will not scale to large databases.\n",
    "A better option is to use pyspark.pandas which provides a Pandas interface\n",
    "to Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8c043e5-9599-40b2-bb94-50336cc8cfd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.pandas import read_csv\n",
    "\n",
    "# Specify the path to the train CSV file in DBFS\n",
    "# Note: When using pyspark.pandas, you don't need to use the '/dbfs/' prefix.\n",
    "# Just use the 'dbfs:/' schema.\n",
    "dbfs_file_path = \"dbfs:/FileStore/shared_uploads/harrison@cs.olemiss.edu/train.csv\"\n",
    "\n",
    "# Read the CSV file into a Pandas-on-Spark DataFrame\n",
    "ps_df = read_csv(dbfs_file_path)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "ps_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "300bd91f-b495-4d09-b0af-63bc43d5f16b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Insert code here for showing a histogram of the age distribution\n",
    "\n",
    "From the Titanic dataset `train.csv` using `matplotlib`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c3bb218-ae86-4983-9cd7-222f7d86a43f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "bin_years = 5.  # each bin spans in bin_years.\n",
    "\n",
    "min_age = df[\"Age\"].min()\n",
    "max_age = df[\"Age\"].max()\n",
    "# arange returns evenly spaced values within a range \n",
    "bins = np.arange(min_age, max_age + bin_years, bin_years)\n",
    "plt.title(\"Histogram of Ages\")\n",
    "plt.xlabel(\"Years\")\n",
    "plt.ylabel(\"Number of Passengers\")\n",
    "plt.hist(df[\"Age\"], bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a2b8798-2348-42b3-b850-71c7fbf91b14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Export the modified notebook\n",
    "\n",
    "Select menu \"File -> Export -> DBC archive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e514892-a1f9-4ddc-8bfc-d6db6ac86b0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Upload your exported notebook to blackboard HW1.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Hello World Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
